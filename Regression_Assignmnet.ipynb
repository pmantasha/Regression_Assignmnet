{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Regression:-"
      ],
      "metadata": {
        "id": "2yaofw99vblf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Theory:-"
      ],
      "metadata": {
        "id": "btTunvFVvdv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "  - Simple Linear Regression is a statistical method used to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered an independent variable (X), and the other is a dependent variable (Y), represented by the formula:\n",
        "\n",
        "     Y = mX + c\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "  - Linearity: The relationship between X and Y is linear.\n",
        "\n",
        "  - Independence: Observations are independent of each other.\n",
        "\n",
        "  - Homoscedasticity: Constant variance of residuals.\n",
        "\n",
        "  - Normality: Residuals should be normally distributed.\n",
        "\n",
        "  - No multicollinearity (though less relevant with only one predictor).\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y = mX + c?\n",
        "  - The coefficient m represents the slope of the regression line. It indicates the change in the dependent variable (Y) for a one-unit increase in the independent variable (X).\n",
        "\n",
        "4. What does the intercept c represent in the equation Y = mX + c?\n",
        "  - The intercept c is the value of Y when X is 0. It represents the point at which the regression line crosses the Y-axis.\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "  - This is the formula for the least squares estimate of the slope.\n",
        "\n",
        "    # m = Σ((X_i - X̄)(Y_i - Ȳ)) / Σ((X_i - X̄)^2)\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        " - To minimize the sum of the squared differences between observed and predicted values. This ensures the best-fitting line.\n",
        "\n",
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "  - R² represents the proportion of the variance in the dependent variable that is predictable from the independent variable.\n",
        "\n",
        "     - R² = 1 means perfect fit\n",
        "\n",
        "     - R² = 0 means the model explains none of the variability\n",
        "\n",
        "8. What is Multiple Linear Regression?\n",
        "  - Multiple Linear Regression models the relationship between a dependent  variable and two or more independent variables, using a linear equation:\n",
        "\n",
        "    Y = β₀ + β₁X₁ + β₂X₂ + ... + βnXn\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "  - Simple Linear Regression uses one independent variable.\n",
        "\n",
        "  - Multiple Linear Regression uses two or more independent variables.\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "  - Linearity\n",
        "\n",
        "  - Independence\n",
        "\n",
        "  - Homoscedasticity\n",
        "\n",
        "  - Normality of residuals\n",
        "\n",
        "  - No multicollinearity among predictors\n",
        "\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        " - Heteroscedasticity means non-constant variance of residuals. It violates regression assumptions and can lead to inefficient estimates and misleading significance tests.\n",
        "\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "  - Remove or combine correlated variables\n",
        "\n",
        "  - Use Principal Component Analysis (PCA)\n",
        "\n",
        "  - Apply Ridge or Lasso Regression\n",
        "\n",
        "  - Check Variance Inflation Factor (VIF) and drop variables with high VIF\n",
        "\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "  - One-hot encoding\n",
        "\n",
        "  - Label encoding\n",
        "\n",
        "  - Binary encoding\n",
        "\n",
        "  - Ordinal encoding (if there’s an order)\n",
        "\n",
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "  - Interaction terms allow the effect of one independent variable to depend on the level of another. This helps model more complex relationships.\n",
        "\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "  - In Simple Linear Regression, the intercept is the predicted value of Y when X = 0.\n",
        "\n",
        "  - In Multiple Linear Regression, the intercept is the predicted value of Y when all X variables are 0, which may not be practically meaningful.\n",
        "\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "  - The slope shows how much Y changes for each unit change in X. It directly affects predictions: higher absolute slopes indicate stronger relationships.\n",
        "\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "  - The intercept provides a baseline value for the dependent variable when all predictors are zero, helping contextualize how other variables shift the outcome.\n",
        "\n",
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        "  - It doesn’t indicate if a model is unbiased\n",
        "\n",
        "  - Can be misleading with overfitting\n",
        "\n",
        "  - Doesn’t account for number of predictors (Adjusted R² does)\n",
        "\n",
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "  - A large standard error suggests high variability in the coefficient estimate, indicating the coefficient may be unreliable or insignificant.\n",
        "\n",
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "  - Identified by funnel shape or uneven spread in residual plots.\n",
        "\n",
        "  - Important to address because it affects standard errors, confidence intervals, and hypothesis testing.\n",
        "\n",
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "  - It suggests that some variables don’t contribute meaningfully to the model, and the high R² might be due to overfitting.\n",
        "\n",
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "  - Ensures equal contribution of predictors\n",
        "\n",
        "  - Helps in interpreting coefficients\n",
        "\n",
        "  - Essential for regularization techniques like Ridge or Lasso\n",
        "\n",
        "23. What is polynomial regression?\n",
        "  - A form of regression where the relationship between independent and dependent variables is modeled as an nth-degree polynomial.\n",
        "\n",
        "24. How does polynomial regression differ from linear regression?\n",
        "  - Linear regression models a straight-line relationship.\n",
        "\n",
        "  - Polynomial regression models curved or non-linear relationships.\n",
        "\n",
        "25. When is polynomial regression used?\n",
        "  - When the data shows non-linear patterns\n",
        "\n",
        "  - To improve model fit without transforming variables\n",
        "\n",
        "26. What is the general equation for polynomial regression?\n",
        "  - # Y = β₀ + β₁X + β₂X² + β₃X³ + ... + βnXⁿ + ε\n",
        "\n",
        "27. Can polynomial regression be applied to multiple variables?\n",
        "  - Yes, it's known as multivariate polynomial regression, where interactions and powers of multiple variables are included.\n",
        "\n",
        "28. What are the limitations of polynomial regression?\n",
        "  - Risk of overfitting\n",
        "\n",
        "  - Interpretability decreases with higher-degree polynomials\n",
        "\n",
        "  - Sensitive to outliers\n",
        "\n",
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "  - Adjusted R²\n",
        "\n",
        "  - Cross-validation (CV)\n",
        "\n",
        "  - AIC/BIC criteria\n",
        "\n",
        "  - Residual plots\n",
        "\n",
        "30. Why is visualization important in polynomial regression?\n",
        "  - Helps identify non-linear patterns\n",
        "\n",
        "  - Assists in choosing degree\n",
        "\n",
        "  - Allows checking overfitting or underfitting visually\n",
        "\n",
        "31. How is polynomial regression implemented in Python?\n",
        " - Using scikit-learn:\n",
        "\n",
        "    from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "\n",
        "    from sklearn.pipeline import make_pipeline\n",
        "\n",
        "    model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "    \n",
        "    model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "Sua7cTtqvgmQ"
      }
    }
  ]
}